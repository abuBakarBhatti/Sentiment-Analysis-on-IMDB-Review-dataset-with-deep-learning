{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89a5198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading and unzipping the IMDB reviews data\n",
    "\n",
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e85f2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There’s also a train/unsup subdirectory in there, which we don’t need. Let’sdelete it:\n",
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543cdd5d",
   "metadata": {},
   "source": [
    "Next, let’s prepare a validation set by setting apart 20% of the training text files in a\n",
    " new directory, aclImdb/val:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe302fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir/'val'\n",
    "train_dir = base_dir/'train'\n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir/category)\n",
    "    files = os.listdir(train_dir/category)\n",
    "    random.Random(1327).shuffle(files)\n",
    "    num_val_samples = int(0.2*len(files))\n",
    "    val_files = files[-num_val_samples: ]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir/category/fname, \n",
    "                    val_dir/category/fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e4c13cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size=32\n",
    "\n",
    "train_dataset = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", \n",
    "    batch_size=batch_size)\n",
    "validation_dataset = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\",\n",
    "     batch_size=batch_size)\n",
    "test_dataset = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\",\n",
    "     batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c54d8998",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3491071444.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Hamza\\AppData\\Local\\Temp\\ipykernel_5304\\3491071444.py\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    print(\"targets datatype\": targets.dtype)\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_dataset:\n",
    "    print('shape of inputs', inputs.shape)\n",
    "    print(\"data type of inputs\", inputs.dtype)\n",
    "    print(\"targets shape\", targets.shape)\n",
    "    print(\"targets datatype\": targets.dtype)\n",
    "    print(\"inputs[0]\", inputs[0])\n",
    "    print(\"inputs[0]\", targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "235c1997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (32,)\n",
      "datatype <dtype: 'string'>\n",
      "targets shape (32,)\n",
      "datatype (32,)\n",
      "inputs[0]: tf.Tensor(b\"I had my doubts about another love story wherein disabled individuals find meaning and redemption through honest communication. And it's still not at the top of my list. But the performances from Helena Bonham Carter and Kenneth Branagh and exemplary, almost stunning, and rescue this from being just another tear-jerker. Carter's depiction of an ALS victim is strong, perhaps even overdone at times (sometimes her dialog dissolves into undistinguishable mutterings). But the overall effect is commendable and rewarding. Branagh may be the perfect compliment to her performance.<br /><br />\", shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_dataset:\n",
    "    print('shape', inputs.shape)\n",
    "    print('datatype', inputs.dtype)\n",
    "    print('targets shape', targets.shape)\n",
    "    print('datatype', targets.shape)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17646631",
   "metadata": {},
   "source": [
    "**Preprocessing our datasets with a TextVectorization layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c74fea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization(max_tokens=20000,\n",
    "                                      ngrams=2,\n",
    "                                      output_mode=\"tf_idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df767b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_dataset = train_dataset.map(lambda x, y:x)\n",
    "text_vectorization.adapt(text_only_dataset)\n",
    "\n",
    "tf_idf_2gram_train_dataset = train_dataset.map(lambda x, y: (text_vectorization(x), y),\n",
    "                                               num_parallel_calls=4)\n",
    "\n",
    "tf_idf_2gram_validation_dataset = validation_dataset.map(lambda x, y: (text_vectorization(x), y),\n",
    "                                                        num_parallel_calls=4)\n",
    "\n",
    "tf_idf_2gram_test_dataset = test_dataset.map(lambda x, y: (text_vectorization(x), y),\n",
    "                                                        num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02b1b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[373.19913     6.9732323   2.8415277 ...   0.          0.\n",
      "   0.       ], shape=(20000,), dtype=float32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for feature, target in tf_idf_2gram_train_dataset:\n",
    "    print(feature[0])\n",
    "    print(target[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29ba84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model building\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens, ))\n",
    "    features = layers.Dense(hidden_dim, activation='relu')(inputs)\n",
    "    features = layers.Dropout(.5)(features)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(features)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    #compilation\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1e7eac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#instantiating the model from get_model \n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bc5776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks = ModelCheckpoint(\"binary_1gram.keras\", \n",
    "#                             save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae924b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 30s 46ms/step - loss: 0.4995 - accuracy: 0.7743 - val_loss: 0.3161 - val_accuracy: 0.8712\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.3599 - accuracy: 0.8501 - val_loss: 0.3518 - val_accuracy: 0.8834\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.3297 - accuracy: 0.8598 - val_loss: 0.3012 - val_accuracy: 0.8844\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.3194 - accuracy: 0.8659 - val_loss: 0.3164 - val_accuracy: 0.8622\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.2932 - accuracy: 0.8753 - val_loss: 0.3152 - val_accuracy: 0.8818\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 7s 10ms/step - loss: 0.2764 - accuracy: 0.8874 - val_loss: 0.3107 - val_accuracy: 0.8892\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.2613 - accuracy: 0.8958 - val_loss: 0.3296 - val_accuracy: 0.8804\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.2518 - accuracy: 0.8995 - val_loss: 0.3479 - val_accuracy: 0.8850\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.2407 - accuracy: 0.9047 - val_loss: 0.3426 - val_accuracy: 0.8824\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.2342 - accuracy: 0.9050 - val_loss: 0.3440 - val_accuracy: 0.8812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x200868e1550>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "model.fit(tf_idf_2gram_train_dataset.cache(),\n",
    "         validation_data=tf_idf_2gram_validation_dataset.cache(),\n",
    "         epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93e66012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 106s 134ms/step - loss: 0.3765 - accuracy: 0.8944\n",
      "Test acc: 0.894\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {model.evaluate(binary_3gram_test_dataset)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270c205e",
   "metadata": {},
   "source": [
    "**So, our model achieved the highest accuracy of 89%** \n",
    "Not much improvement. But with other datasets it might be helpful in gaining more 1% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee15a430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f4d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbd0c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9297ef16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ad8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7854930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c600fb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c1d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f58fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1db105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e925c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c52109d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faa07ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
