{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89a5198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading and unzipping the IMDB reviews data\n",
    "\n",
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e85f2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There’s also a train/unsup subdirectory in there, which we don’t need. Let’sdelete it:\n",
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543cdd5d",
   "metadata": {},
   "source": [
    "Next, let’s prepare a validation set by setting apart 20% of the training text files in a\n",
    " new directory, aclImdb/val:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe302fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir/'val'\n",
    "train_dir = base_dir/'train'\n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir/category)\n",
    "    files = os.listdir(train_dir/category)\n",
    "    random.Random(1327).shuffle(files)\n",
    "    num_val_samples = int(0.2*len(files))\n",
    "    val_files = files[-num_val_samples: ]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir/category/fname, \n",
    "                    val_dir/category/fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23ad18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", \n",
    "    batch_size=batch_size)\n",
    "\n",
    "validation_dataset = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size)\n",
    "\n",
    "test_dataset = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "235c1997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (32,)\n",
      "datatype <dtype: 'string'>\n",
      "targets shape (32,)\n",
      "datatype (32,)\n",
      "inputs[0]: tf.Tensor(b'Another turgid action/adventure flick from the Quinn Martin Productions factory. Roy Thinnes plays undercover agent Diamond Head (Mr. Head, to you), working for his G-Man handler \"Aunt Mary\", looking for \"Tree\", who\\'s on a mission to...well, just watch the movie. <br /><br />This one deserved and got the full MST3K sendup. As the boys and various reviewers have pointed out, the movie \"Fargo\" had more Hawaiian locations than this film. Apparently shot on a puny budget, this movie highlights Hawaii\\'s broken-down dive shops, gas stations, and cheapo hotels. Zulu -- later to star as Kono in Hawaii-Five-O -- appears as Thinnes\\' lumpy, inept sidekick, while France Nguyen models the Jenny Craig diet gone horribly wrong. Others sharing the flickering screen include a drunken Richard Harris knockoff, a George Takai imitator, a not-so-smart hit-man with sprayed-on Sansabelt slacks, and the villain \"Tree\", sporting a veddy British accent. You can pretty much figure out the plot halfway through the opening credits, but relax--just enjoy the giddy mediocrity of this 70\\'s movie-of-the-week.<br /><br />Whenever I think of this movie (and I think of this movie often), I catch myself humming the theme, written for flute and tuba...no one knows why. <br /><br />Trivia note--Diamond Head was directed by Jeannot Szwarc, one of three contract directors at Universal who would go on to make much bigger films, in his case Jaws 2. The others were John Badham (War Games), and a young fellow named Steven Spielberg...', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor([0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1], shape=(32,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_dataset:\n",
    "    print('shape', inputs.shape)\n",
    "    print('datatype', inputs.dtype)\n",
    "    print('targets shape', targets.shape)\n",
    "    print('datatype', targets.shape)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17646631",
   "metadata": {},
   "source": [
    "**Preprocessing our datasets with a TextVectorization layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9297ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization\n",
    "\n",
    "max_tokens= 20000\n",
    "max_length= 600\n",
    "\n",
    "text_vectorization = TextVectorization(max_tokens=max_tokens,\n",
    "                                      output_mode='int',\n",
    "                                      output_sequence_length=max_length\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "450ad8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_train_dataset = train_dataset.map(lambda x, y: x)\n",
    "\n",
    "text_vectorization.adapt(text_only_train_dataset)\n",
    "\n",
    "int_train_dataset = train_dataset.map(lambda x, y: (text_vectorization(x), y),\n",
    "                                     num_parallel_calls=4)\n",
    "\n",
    "int_validation_dataset = validation_dataset.map(lambda x, y: (text_vectorization(x),y),\n",
    "                                               num_parallel_calls=4)\n",
    "\n",
    "int_test_dataset = test_dataset.map(lambda x, y: (text_vectorization(x),y),\n",
    "                                               num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7854930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[   10   204   252 14963  5928   237   940     3    11    19 15901    12\n",
      "    15   155   731   292    11     7     4  5795    19    12   207    53\n",
      "   166    10   282     9    78     9    44  4237     8     9    18    28\n",
      "   982   202    98   290    63    30    57  2329    33  9864     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0], shape=(600,), dtype=int64)\n",
      "tf.Tensor([0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1], shape=(32,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in int_train_dataset:\n",
    "    print(inputs[0])\n",
    "    print(targets)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4851334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 64)                73984     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5194049 (19.81 MB)\n",
      "Trainable params: 5194049 (19.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# training model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(None, ), dtype='int64')\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs) #embedding layer to convert integer indices into vectors\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x= layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "#model compilation\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d88ef5d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 659s 1s/step - loss: 0.5363 - accuracy: 0.7305 - val_loss: 0.4699 - val_accuracy: 0.8126\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 678s 1s/step - loss: 0.3595 - accuracy: 0.8624 - val_loss: 0.3213 - val_accuracy: 0.8746\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 388s 619ms/step - loss: 0.2931 - accuracy: 0.8929 - val_loss: 0.3240 - val_accuracy: 0.8802\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 411s 658ms/step - loss: 0.2476 - accuracy: 0.9130 - val_loss: 0.3165 - val_accuracy: 0.8796\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 429s 686ms/step - loss: 0.2111 - accuracy: 0.9291 - val_loss: 0.4887 - val_accuracy: 0.8498\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 369s 591ms/step - loss: 0.1817 - accuracy: 0.9387 - val_loss: 0.3489 - val_accuracy: 0.8494\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 393s 629ms/step - loss: 0.1575 - accuracy: 0.9497 - val_loss: 0.3503 - val_accuracy: 0.8776\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 391s 624ms/step - loss: 0.1450 - accuracy: 0.9563 - val_loss: 0.3773 - val_accuracy: 0.8770\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 355s 568ms/step - loss: 0.1220 - accuracy: 0.9618 - val_loss: 0.3691 - val_accuracy: 0.8742\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 352s 563ms/step - loss: 0.1010 - accuracy: 0.9700 - val_loss: 0.4068 - val_accuracy: 0.8702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1fc95524c70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model training\n",
    "model.fit(int_train_dataset,\n",
    "         validation_data=int_validation_dataset,\n",
    "         epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f74acff",
   "metadata": {},
   "source": [
    "**Maximum accuracy is 88%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c600fb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 278s 354ms/step - loss: 0.5003 - accuracy: 0.8421\n",
      "Test acc: 0.842\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {model.evaluate(int_test_dataset)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d22a1d6",
   "metadata": {},
   "source": [
    "**Okay, It's accuracy is low. We did not trained it on full data as we truncated each review after 600 words. However, thanks to embedding layer, now our model has to process only 256 dimentional vectors insteas of 20000 thousand as we saw in previous notebook where one-hot encoding was applied.**\n",
    "\n",
    "In next notebook, we shall apply 'masking' so that LSTM only process non-zero indices and skip those indices where there is 0. \n",
    "It is done by setting \"mask-zero=True\" in embedding layer. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f58fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1db105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e925c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c52109d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faa07ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
